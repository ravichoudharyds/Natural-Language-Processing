{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ravichoudharyds/Natural-Language-Processing/blob/master/HW1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZKMekmNeAOd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnTmnbFMeClq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "2ef3e061-0c74-43b3-f062-21e99a53a204"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TCB_YQLeDbR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ed63dc11-f164-4a05-ce06-4cfe9999c6cd"
      },
      "source": [
        "# Let's write the tokenization function \n",
        "\n",
        "import spacy\n",
        "import string\n",
        "import pickle as pkl\n",
        "import torch\n",
        "\n",
        "# Load English tokenizer, tagger, parser, NER and word vectors\n",
        "tokenizer = spacy.load('en_core_web_sm')\n",
        "punctuations = string.punctuation\n",
        "\n",
        "# lowercase and remove punctuation\n",
        "def tokenize(sent):\n",
        "    tokens = tokenizer(sent)\n",
        "    return [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
        "\n",
        "# Example\n",
        "tokens = tokenize(u'Apple is looking at buying U.K. startup for $1 billion.')\n",
        "print (tokens)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['apple', 'is', 'looking', 'at', 'buying', 'u.k.', 'startup', 'for', '1', 'billion']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlDPVn9_j83N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is the code cell that tokenizes train/val/test datasets\n",
        "# However it takes about 15-20 minutes to run it\n",
        "# For convinience we have provided the preprocessed datasets\n",
        "# Please see the next code cell\n",
        "\n",
        "\n",
        "def tokenize_dataset(dataset):\n",
        "    sentence1_tokens_dataset = []\n",
        "    sentence2_tokens_dataset = []\n",
        "    # we are keeping track of all tokens in dataset \n",
        "    # in order to create vocabulary later\n",
        "    all_tokens = []\n",
        "    label_list = []\n",
        "    for sample in dataset:\n",
        "        sentence1, sentence2, label = sample.split(\"\\t\")\n",
        "        if label[-1] == '\\n':\n",
        "            label = label[:-1]\n",
        "        sentence1_tokens = tokenize(sentence1)\n",
        "        sentence2_tokens = tokenize(sentence2)\n",
        "        sentence1_tokens_dataset.append(sentence1_tokens)\n",
        "        sentence2_tokens_dataset.append(sentence2_tokens)\n",
        "        label_list.append(label)\n",
        "        all_tokens += sentence1_tokens\n",
        "        all_tokens += sentence2_tokens\n",
        "\n",
        "    return sentence1_tokens_dataset, sentence2_tokens_dataset, label_list, all_tokens\n",
        "\n",
        "#val set tokens\n",
        "print (\"Tokenizing val data\")\n",
        "val_data = open(\"/content/drive/My Drive/NLP_HW/HW1/snli_val.tsv\",\"r\")\n",
        "sentence1_val_tokens_dataset, sentence2_val_tokens_dataset, snli_val_label_list, _ = tokenize_dataset(val_data)\n",
        "pkl.dump(sentence1_val_tokens_dataset, open(\"/content/drive/My Drive/NLP_HW/HW1/snli_val_sentence1_tokens.p\", \"wb\"))\n",
        "pkl.dump(sentence2_val_tokens_dataset, open(\"/content/drive/My Drive/NLP_HW/HW1/snli_val_sentence2_tokens.p\", \"wb\"))\n",
        "pkl.dump(snli_val_label_list, open(\"/content/drive/My Drive/NLP_HW/HW1/snli_val_label_list.p\", \"wb\"))\n",
        "\n",
        "#train set tokens\n",
        "print (\"Tokenizing train data\")\n",
        "train_data = open(\"/content/drive/My Drive/NLP_HW/HW1/snli_train.tsv\",\"r\")\n",
        "sentence1_train_tokens_dataset, sentence2_train_tokens_dataset, snli_train_label_list, snli_train_tokens = tokenize_dataset(train_data)\n",
        "pkl.dump(sentence1_train_tokens_dataset, open(\"/content/drive/My Drive/NLP_HW/HW1/snli_train_sentence1_tokens.p\", \"wb\"))\n",
        "pkl.dump(sentence2_train_tokens_dataset, open(\"/content/drive/My Drive/NLP_HW/HW1/snli_train_sentence2_tokens.p\", \"wb\"))\n",
        "pkl.dump(snli_train_label_list, open(\"/content/drive/My Drive/NLP_HW/HW1/snli_train_label_list.p\", \"wb\"))\n",
        "pkl.dump(snli_train_tokens, open(\"/content/drive/My Drive/NLP_HW/HW1/snli_train_tokens.p\", \"wb\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wVSxbwpYX8t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence1_train_tokens_dataset = pkl.load(open(\"/content/drive/My Drive/NLP_HW/HW1/snli_train_sentence1_tokens.p\", \"rb\"))\n",
        "sentence2_train_tokens_dataset = pkl.load(open(\"/content/drive/My Drive/NLP_HW/HW1/snli_train_sentence2_tokens.p\", \"rb\"))\n",
        "snli_train_label_list = pkl.load(open(\"/content/drive/My Drive/NLP_HW/HW1/snli_train_label_list.p\", \"rb\"))\n",
        "snli_train_tokens = pkl.load(open(\"/content/drive/My Drive/NLP_HW/HW1/snli_train_tokens.p\", \"rb\"))\n",
        "\n",
        "sentence1_val_tokens_dataset = pkl.load(open(\"/content/drive/My Drive/NLP_HW/HW1/snli_val_sentence1_tokens.p\", \"rb\"))\n",
        "sentence2_val_tokens_dataset = pkl.load(open(\"/content/drive/My Drive/NLP_HW/HW1/snli_val_sentence2_tokens.p\", \"rb\"))\n",
        "snli_val_label_list = pkl.load(open(\"/content/drive/My Drive/NLP_HW/HW1/snli_val_label_list.p\", \"rb\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdCY-uhm9Dms",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_target = []\n",
        "val_target = []\n",
        "target_dict = {'contradiction':0, 'entailment':1, 'neutral':2}\n",
        "for target in snli_train_label_list[1:]:\n",
        "    train_target.append(target_dict[target])\n",
        "for target in snli_val_label_list[1:]:\n",
        "    val_target.append(target_dict[target])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rgy4n38BpGjZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_SENTENCE_LENGTH = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuPZwyp_pGsu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SNLIDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
        "    Note that this class inherits torch.utils.data.Dataset\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, sentence1_list, sentence2_list, target_list):\n",
        "        \"\"\"\n",
        "        @param data_list: list of newsgroup tokens \n",
        "        @param target_list: list of newsgroup targets \n",
        "\n",
        "        \"\"\"\n",
        "        self.sentence1_list = sentence1_list\n",
        "        self.sentence2_list = sentence2_list\n",
        "        self.target_list = target_list\n",
        "        assert (len(self.sentence1_list) == len(self.target_list))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.target_list)\n",
        "        \n",
        "    def __getitem__(self, key):\n",
        "        \"\"\"\n",
        "        Triggered when you call dataset[i]\n",
        "        \"\"\"\n",
        "        \n",
        "        token1_idx = self.sentence1_list[key][:MAX_SENTENCE_LENGTH]\n",
        "        token2_idx = self.sentence2_list[key][:MAX_SENTENCE_LENGTH]\n",
        "        label = self.target_list[key]\n",
        "        return [token1_idx, token2_idx, len(token1_idx), len(token2_idx), label]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKhaVJj3kb2J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "# save index 0 for unk and 1 for pad\n",
        "PAD_IDX = 0\n",
        "UNK_IDX = 1\n",
        "\n",
        "def build_vocab(all_tokens, max_vocab_size = 10**4):\n",
        "    # Returns:\n",
        "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
        "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
        "    token_counter = Counter(all_tokens)\n",
        "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
        "    id2token = list(vocab)\n",
        "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
        "    id2token = ['<pad>', '<unk>'] + id2token\n",
        "    token2id['<pad>'] = PAD_IDX \n",
        "    token2id['<unk>'] = UNK_IDX\n",
        "    return token2id, id2token\n",
        "\n",
        "token2id, id2token = build_vocab(snli_train_tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bn1DaZXHkaeR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "e6afaaef-80f7-4a9c-e23a-0ad06c595cda"
      },
      "source": [
        "# Lets check the dictionary by loading random token from it\n",
        "import random\n",
        "\n",
        "random_token_id = random.randint(0, len(id2token)-1)\n",
        "random_token = id2token[random_token_id]\n",
        "\n",
        "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
        "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token id 9845 ; token picasso\n",
            "Token picasso; token id 9845\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKzeSf6Vkjp8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "8f830b4f-5ff1-4381-c4c0-40ee1dc410d5"
      },
      "source": [
        "# convert token to id in the dataset\n",
        "def token2index_dataset(tokens_data):\n",
        "    indices_data = []\n",
        "    for tokens in tokens_data:\n",
        "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
        "        indices_data.append(index_list)\n",
        "    return indices_data\n",
        "\n",
        "sent1_train_indices = token2index_dataset(sentence1_train_tokens_dataset)\n",
        "sent2_train_indices = token2index_dataset(sentence2_train_tokens_dataset)\n",
        "sent1_val_indices = token2index_dataset(sentence1_val_tokens_dataset)\n",
        "sent2_val_indices = token2index_dataset(sentence2_val_tokens_dataset)\n",
        "\n",
        "# double checking\n",
        "print (\"Train dataset size is {}\".format(len(sent1_train_indices)))\n",
        "print (\"Train dataset size is {}\".format(len(sentence1_train_tokens_dataset)))\n",
        "print (\"Train dataset size is {}\".format(len(sent2_train_indices)))\n",
        "print (\"Train dataset size is {}\".format(len(sentence2_train_tokens_dataset)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train dataset size is 100001\n",
            "Train dataset size is 100001\n",
            "Train dataset size is 100001\n",
            "Train dataset size is 100001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lqtoe8DbpRQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = SNLIDataset(sent1_train_indices[1:], sent2_train_indices[1:], train_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3yK-WnXqLNY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def snli_collate_func(batch):\n",
        "    \"\"\"\n",
        "    Customized function for DataLoader that dynamically pads the batch so that all \n",
        "    data have the same length\n",
        "    \"\"\"\n",
        "    sent1_list = []\n",
        "    sent2_list = []\n",
        "    label_list = []\n",
        "    length_list_sent1 = []\n",
        "    length_list_sent2 = []\n",
        "    #print(\"collate batch: \", batch[0][0])\n",
        "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
        "    for datum in batch:\n",
        "        label_list.append(datum[4])\n",
        "        length_list_sent1.append(datum[2])\n",
        "        length_list_sent2.append(datum[3])\n",
        "    # padding\n",
        "    for datum in batch:\n",
        "        padded_vec1 = np.pad(np.array(datum[0]), \n",
        "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[2])), \n",
        "                                mode=\"constant\", constant_values=0)\n",
        "        sent1_list.append(padded_vec1)\n",
        "        padded_vec2 = np.pad(np.array(datum[1]), \n",
        "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[3])), \n",
        "                                mode=\"constant\", constant_values=0)\n",
        "        sent2_list.append(padded_vec2)\n",
        "    return [torch.from_numpy(np.array(sent1_list)), torch.from_numpy(np.array(sent2_list)), torch.LongTensor(length_list_sent1),torch.LongTensor(length_list_sent2), torch.LongTensor(label_list)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNt9PZYqsGt5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 256\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           collate_fn=snli_collate_func,\n",
        "                                           shuffle=True)\n",
        "\n",
        "val_dataset = SNLIDataset(sent1_val_indices[1:], sent2_val_indices[1:], val_target)\n",
        "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           collate_fn=snli_collate_func,\n",
        "                                           shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuBtdxsggR4J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First import torch related libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BagOfWords_Model(nn.Module):\n",
        "    \"\"\"\n",
        "    BagOfWords classification model\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, emb_dim, nn_ind = False, interaction_type = 'cat'):\n",
        "        \"\"\"\n",
        "        @param vocab_size: size of the vocabulary. \n",
        "        @param emb_dim: size of the word embedding\n",
        "        \"\"\"\n",
        "        super(BagOfWords_Model, self).__init__()\n",
        "        # pay attention to padding_idx \n",
        "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
        "        self.nn_ind = nn_ind\n",
        "        self.interaction_type = interaction_type\n",
        "        if self.interaction_type =='cat':\n",
        "            if nn_ind:\n",
        "                self.linear1 = nn.Linear(2*emb_dim, emb_dim)\n",
        "                self.relu1 = nn.ReLU()\n",
        "                self.linear2 = nn.Linear(emb_dim, int(emb_dim/2))\n",
        "                self.relu2 = nn.ReLU()\n",
        "                self.linear3 = nn.Linear(int(emb_dim/2), 3)\n",
        "            else:\n",
        "                self.linear = nn.Linear(2*emb_dim,3)\n",
        "        else:\n",
        "            if nn_ind:\n",
        "                self.linear1 = nn.Linear(emb_dim, int(emb_dim/2))\n",
        "                self.relu1 = nn.ReLU()\n",
        "                self.linear2 = nn.Linear(int(emb_dim/2),int(emb_dim/4))\n",
        "                self.relu2 = nn.ReLU()\n",
        "                self.linear3 = nn.Linear(int(emb_dim/4), 3)\n",
        "            else:\n",
        "                self.linear = nn.Linear(emb_dim,3)\n",
        "    \n",
        "    def interaction_func(self, sent1, sent2):\n",
        "        if self.interaction_type == 'sum':\n",
        "            out = sent1.float() + sent2.float()\n",
        "        elif self.interaction_type == 'cat':\n",
        "            out = torch.cat((sent1.float(),sent2.float()),dim=1)\n",
        "        else:\n",
        "            out = sent1.float()*sent2.float()\n",
        "        return out\n",
        "    \n",
        "    def forward(self, sent1, sent2, length1, length2):\n",
        "        \"\"\"\n",
        "        \n",
        "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
        "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
        "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
        "            length of each sentences in the data.\n",
        "        \"\"\"\n",
        "        out1 = self.embed(sent1)\n",
        "        out1 = torch.sum(out1, dim=1)\n",
        "        out1 /= length1.view(length1.size()[0],1).expand_as(out1).float()\n",
        "        out2 = self.embed(sent2)\n",
        "        out2 = torch.sum(out2, dim=1)\n",
        "        out2 /= length2.view(length2.size()[0],1).expand_as(out2).float()\n",
        "        out = self.interaction_func(out1, out2)\n",
        "\n",
        "        # return logits\n",
        "        \n",
        "        if self.nn_ind:\n",
        "            out = self.relu1(self.linear1(out))\n",
        "            out = self.relu2(self.linear2(out))\n",
        "            out = self.linear3(out)            \n",
        "        else:\n",
        "            out = self.linear(out)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35k7xEMg30r7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 10 # number epoch to train\n",
        "\n",
        "# Function for testing the model\n",
        "def test_model(loader, model, criterion):\n",
        "    \"\"\"\n",
        "    Help function that tests the model's performance on a dataset\n",
        "    @param: loader - data loader for the dataset to test against\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    total_loss = 0\n",
        "    total = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for sent1, sent2, length1, length2, labels in loader:\n",
        "            sent1_batch, sent2_batch, length1_batch, length2_batch, label_batch = sent1.to(device), sent2.to(device), length1.to(device), length2.to(device), labels.to(device)\n",
        "            outputs = F.softmax(model(sent1_batch, sent2_batch, length1_batch, length2_batch), dim=1).to(device)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            loss = criterion(outputs, label_batch)\n",
        "            total_loss += loss.item()\n",
        "            total += label_batch.size(0)\n",
        "            correct += predicted.eq(label_batch.view_as(predicted)).sum().item()\n",
        "    return (100 * correct / total), (total_loss/total)\n",
        "\n",
        "def train_model(train_loader, val_loader, model, optimizer, criterion, num_epochs):\n",
        "    train_acc_list = []\n",
        "    train_loss_list = []\n",
        "    val_acc_list = []\n",
        "    val_loss_list = []\n",
        "    for epoch in range(num_epochs):\n",
        "        for sent1, sent2, length1, length2, labels in train_loader:\n",
        "            model.train()\n",
        "            sent1_batch, sent2_batch, length1_batch, length2_batch, label_batch = sent1.to(device), sent2.to(device), length1.to(device), length2.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(sent1_batch, sent2_batch, length1_batch, length2_batch)\n",
        "            loss = criterion(outputs, label_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        train_acc, train_loss = test_model(train_loader, model, criterion)\n",
        "        val_acc, val_loss = test_model(val_loader, model, criterion)\n",
        "        train_acc_list.append(train_acc)\n",
        "        train_loss_list.append(train_loss)\n",
        "        val_acc_list.append(val_acc)\n",
        "        val_loss_list.append(val_loss)\n",
        "        print(\"Epoch:{}, Validation Accuracy:{}, Training Acc: {}\".format(epoch+1, val_acc, train_acc))\n",
        "    return train_acc_list, train_loss_list, val_acc_list, val_loss_list\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9t3kjUzCJBjb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a4b54b9c-cfdc-4d6d-a57f-f0fd50e178d2"
      },
      "source": [
        "from itertools import product\n",
        "\n",
        "vocab_size_list = [1000, 5000, 10000]\n",
        "emb_dim_list = [100, 150, 200]\n",
        "nn_ind_list = [0, 1]\n",
        "interaction_list = ['cat','sum','mult']\n",
        "results_dict = {}\n",
        "\n",
        "for vocab_size, emb_dim, nn_ind, interaction_type in product(vocab_size_list, emb_dim_list, nn_ind_list, interaction_list):\n",
        "    token2id, id2token = build_vocab(snli_train_tokens, max_vocab_size = 2*10**4)\n",
        "    sent1_train_indices = token2index_dataset(sentence1_train_tokens_dataset)\n",
        "    sent2_train_indices = token2index_dataset(sentence2_train_tokens_dataset)\n",
        "    sent1_val_indices = token2index_dataset(sentence1_val_tokens_dataset)\n",
        "    sent2_val_indices = token2index_dataset(sentence2_val_tokens_dataset)\n",
        "    train_dataset = SNLIDataset(sent1_train_indices[1:], sent2_train_indices[1:], train_target)\n",
        "    BATCH_SIZE = 256\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                               batch_size=BATCH_SIZE,\n",
        "                                               collate_fn=snli_collate_func,\n",
        "                                               shuffle=True)\n",
        "\n",
        "    val_dataset = SNLIDataset(sent1_val_indices[1:], sent2_val_indices[1:], val_target)\n",
        "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
        "                                               batch_size=BATCH_SIZE,\n",
        "                                               collate_fn=snli_collate_func,\n",
        "                                               shuffle=True)\n",
        "    model = BagOfWords_Model(len(id2token), emb_dim, nn_ind=nn_ind, interaction_type = interaction_type).to(device)\n",
        "    criterion = torch.nn.CrossEntropyLoss(reduction='sum')  \n",
        "\n",
        "    learning_rate = 0.01\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    combination_dict_key = str(vocab_size) + \", \" + str(emb_dim) + \", \" + str(nn_ind) + \", \" + str(interaction_type)\n",
        "    print(combination_dict_key)\n",
        "    train_acc_list, train_loss_list, val_acc_list, val_loss_list = train_model(train_loader, val_loader, model, optimizer, criterion, num_epochs)\n",
        "    results_dict[combination_dict_key] = [train_acc_list, train_loss_list, val_acc_list, val_loss_list]\n",
        "pkl.dump(results_dict, open(\"/content/drive/My Drive/NLP_HW/HW1/Hyperparameter_Search_Results_Dictionary.p\", \"wb\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000, 100, 0, cat\n",
            "Epoch:1, Validation Accuracy:62.5, Training Acc: 66.258\n",
            "Epoch:2, Validation Accuracy:62.1, Training Acc: 69.704\n",
            "Epoch:3, Validation Accuracy:61.3, Training Acc: 71.624\n",
            "Epoch:4, Validation Accuracy:59.6, Training Acc: 73.599\n",
            "Epoch:5, Validation Accuracy:61.4, Training Acc: 74.65\n",
            "Epoch:6, Validation Accuracy:61.9, Training Acc: 74.926\n",
            "Epoch:7, Validation Accuracy:60.8, Training Acc: 75.855\n",
            "Epoch:8, Validation Accuracy:60.8, Training Acc: 76.117\n",
            "Epoch:9, Validation Accuracy:61.5, Training Acc: 76.605\n",
            "Epoch:10, Validation Accuracy:60.0, Training Acc: 76.878\n",
            "1000, 100, 0, sum\n",
            "Epoch:1, Validation Accuracy:56.1, Training Acc: 59.501\n",
            "Epoch:2, Validation Accuracy:56.4, Training Acc: 62.795\n",
            "Epoch:3, Validation Accuracy:57.1, Training Acc: 64.06\n",
            "Epoch:4, Validation Accuracy:55.8, Training Acc: 65.05\n",
            "Epoch:5, Validation Accuracy:55.7, Training Acc: 66.006\n",
            "Epoch:6, Validation Accuracy:56.2, Training Acc: 66.378\n",
            "Epoch:7, Validation Accuracy:55.0, Training Acc: 66.648\n",
            "Epoch:8, Validation Accuracy:56.5, Training Acc: 66.989\n",
            "Epoch:9, Validation Accuracy:55.9, Training Acc: 66.719\n",
            "Epoch:10, Validation Accuracy:55.2, Training Acc: 67.178\n",
            "1000, 100, 0, mult\n",
            "Epoch:1, Validation Accuracy:62.3, Training Acc: 68.585\n",
            "Epoch:2, Validation Accuracy:64.7, Training Acc: 80.41\n",
            "Epoch:3, Validation Accuracy:65.1, Training Acc: 87.838\n",
            "Epoch:4, Validation Accuracy:63.1, Training Acc: 92.513\n",
            "Epoch:5, Validation Accuracy:63.7, Training Acc: 95.256\n",
            "Epoch:6, Validation Accuracy:63.7, Training Acc: 96.916\n",
            "Epoch:7, Validation Accuracy:62.6, Training Acc: 98.08\n",
            "Epoch:8, Validation Accuracy:62.9, Training Acc: 98.765\n",
            "Epoch:9, Validation Accuracy:62.9, Training Acc: 99.176\n",
            "Epoch:10, Validation Accuracy:62.9, Training Acc: 99.455\n",
            "1000, 100, 1, cat\n",
            "Epoch:1, Validation Accuracy:63.6, Training Acc: 68.874\n",
            "Epoch:2, Validation Accuracy:65.4, Training Acc: 75.944\n",
            "Epoch:3, Validation Accuracy:66.2, Training Acc: 81.28\n",
            "Epoch:4, Validation Accuracy:67.4, Training Acc: 85.886\n",
            "Epoch:5, Validation Accuracy:64.5, Training Acc: 89.168\n",
            "Epoch:6, Validation Accuracy:65.2, Training Acc: 91.713\n",
            "Epoch:7, Validation Accuracy:65.0, Training Acc: 93.446\n",
            "Epoch:8, Validation Accuracy:64.9, Training Acc: 94.544\n",
            "Epoch:9, Validation Accuracy:65.1, Training Acc: 95.762\n",
            "Epoch:10, Validation Accuracy:64.1, Training Acc: 96.326\n",
            "1000, 100, 1, sum\n",
            "Epoch:1, Validation Accuracy:56.8, Training Acc: 61.068\n",
            "Epoch:2, Validation Accuracy:60.0, Training Acc: 67.137\n",
            "Epoch:3, Validation Accuracy:58.7, Training Acc: 71.206\n",
            "Epoch:4, Validation Accuracy:59.7, Training Acc: 75.956\n",
            "Epoch:5, Validation Accuracy:59.5, Training Acc: 78.79\n",
            "Epoch:6, Validation Accuracy:59.1, Training Acc: 81.617\n",
            "Epoch:7, Validation Accuracy:59.1, Training Acc: 84.084\n",
            "Epoch:8, Validation Accuracy:58.9, Training Acc: 85.651\n",
            "Epoch:9, Validation Accuracy:59.0, Training Acc: 87.4\n",
            "Epoch:10, Validation Accuracy:57.3, Training Acc: 88.666\n",
            "1000, 100, 1, mult\n",
            "Epoch:1, Validation Accuracy:60.4, Training Acc: 67.337\n",
            "Epoch:2, Validation Accuracy:64.3, Training Acc: 79.163\n",
            "Epoch:3, Validation Accuracy:64.1, Training Acc: 87.209\n",
            "Epoch:4, Validation Accuracy:62.3, Training Acc: 91.36\n",
            "Epoch:5, Validation Accuracy:61.3, Training Acc: 94.249\n",
            "Epoch:6, Validation Accuracy:60.0, Training Acc: 95.927\n",
            "Epoch:7, Validation Accuracy:60.4, Training Acc: 96.774\n",
            "Epoch:8, Validation Accuracy:60.5, Training Acc: 97.773\n",
            "Epoch:9, Validation Accuracy:59.8, Training Acc: 98.412\n",
            "Epoch:10, Validation Accuracy:60.6, Training Acc: 98.714\n",
            "1000, 150, 0, cat\n",
            "Epoch:1, Validation Accuracy:61.8, Training Acc: 66.116\n",
            "Epoch:2, Validation Accuracy:62.3, Training Acc: 69.723\n",
            "Epoch:3, Validation Accuracy:60.9, Training Acc: 71.911\n",
            "Epoch:4, Validation Accuracy:60.9, Training Acc: 73.573\n",
            "Epoch:5, Validation Accuracy:61.3, Training Acc: 74.646\n",
            "Epoch:6, Validation Accuracy:60.0, Training Acc: 75.216\n",
            "Epoch:7, Validation Accuracy:61.4, Training Acc: 75.294\n",
            "Epoch:8, Validation Accuracy:61.0, Training Acc: 76.292\n",
            "Epoch:9, Validation Accuracy:61.1, Training Acc: 76.593\n",
            "Epoch:10, Validation Accuracy:59.7, Training Acc: 76.794\n",
            "1000, 150, 0, sum\n",
            "Epoch:1, Validation Accuracy:54.5, Training Acc: 58.464\n",
            "Epoch:2, Validation Accuracy:57.4, Training Acc: 62.517\n",
            "Epoch:3, Validation Accuracy:55.9, Training Acc: 63.928\n",
            "Epoch:4, Validation Accuracy:56.8, Training Acc: 65.432\n",
            "Epoch:5, Validation Accuracy:54.5, Training Acc: 65.345\n",
            "Epoch:6, Validation Accuracy:56.0, Training Acc: 66.24\n",
            "Epoch:7, Validation Accuracy:56.7, Training Acc: 66.598\n",
            "Epoch:8, Validation Accuracy:56.5, Training Acc: 66.547\n",
            "Epoch:9, Validation Accuracy:55.2, Training Acc: 67.152\n",
            "Epoch:10, Validation Accuracy:54.8, Training Acc: 67.174\n",
            "1000, 150, 0, mult\n",
            "Epoch:1, Validation Accuracy:63.4, Training Acc: 70.092\n",
            "Epoch:2, Validation Accuracy:65.3, Training Acc: 82.716\n",
            "Epoch:3, Validation Accuracy:64.3, Training Acc: 90.566\n",
            "Epoch:4, Validation Accuracy:63.6, Training Acc: 94.808\n",
            "Epoch:5, Validation Accuracy:63.8, Training Acc: 97.201\n",
            "Epoch:6, Validation Accuracy:64.3, Training Acc: 98.364\n",
            "Epoch:7, Validation Accuracy:63.7, Training Acc: 99.088\n",
            "Epoch:8, Validation Accuracy:63.5, Training Acc: 99.409\n",
            "Epoch:9, Validation Accuracy:63.2, Training Acc: 99.639\n",
            "Epoch:10, Validation Accuracy:62.9, Training Acc: 99.757\n",
            "1000, 150, 1, cat\n",
            "Epoch:1, Validation Accuracy:64.8, Training Acc: 69.645\n",
            "Epoch:2, Validation Accuracy:64.6, Training Acc: 75.896\n",
            "Epoch:3, Validation Accuracy:64.6, Training Acc: 81.22\n",
            "Epoch:4, Validation Accuracy:65.1, Training Acc: 86.509\n",
            "Epoch:5, Validation Accuracy:64.8, Training Acc: 90.07\n",
            "Epoch:6, Validation Accuracy:64.2, Training Acc: 92.145\n",
            "Epoch:7, Validation Accuracy:64.7, Training Acc: 94.597\n",
            "Epoch:8, Validation Accuracy:65.7, Training Acc: 95.695\n",
            "Epoch:9, Validation Accuracy:66.0, Training Acc: 96.713\n",
            "Epoch:10, Validation Accuracy:64.8, Training Acc: 97.452\n",
            "1000, 150, 1, sum\n",
            "Epoch:1, Validation Accuracy:57.3, Training Acc: 62.041\n",
            "Epoch:2, Validation Accuracy:59.4, Training Acc: 67.647\n",
            "Epoch:3, Validation Accuracy:58.5, Training Acc: 72.73\n",
            "Epoch:4, Validation Accuracy:59.1, Training Acc: 77.066\n",
            "Epoch:5, Validation Accuracy:58.3, Training Acc: 80.601\n",
            "Epoch:6, Validation Accuracy:59.1, Training Acc: 83.792\n",
            "Epoch:7, Validation Accuracy:58.1, Training Acc: 86.02\n",
            "Epoch:8, Validation Accuracy:58.9, Training Acc: 88.143\n",
            "Epoch:9, Validation Accuracy:58.4, Training Acc: 89.95\n",
            "Epoch:10, Validation Accuracy:58.0, Training Acc: 91.012\n",
            "1000, 150, 1, mult\n",
            "Epoch:1, Validation Accuracy:62.7, Training Acc: 69.2\n",
            "Epoch:2, Validation Accuracy:65.2, Training Acc: 82.3\n",
            "Epoch:3, Validation Accuracy:65.4, Training Acc: 90.113\n",
            "Epoch:4, Validation Accuracy:64.3, Training Acc: 94.341\n",
            "Epoch:5, Validation Accuracy:65.5, Training Acc: 96.057\n",
            "Epoch:6, Validation Accuracy:63.8, Training Acc: 97.408\n",
            "Epoch:7, Validation Accuracy:64.5, Training Acc: 98.333\n",
            "Epoch:8, Validation Accuracy:62.5, Training Acc: 98.717\n",
            "Epoch:9, Validation Accuracy:63.2, Training Acc: 98.97\n",
            "Epoch:10, Validation Accuracy:62.9, Training Acc: 99.118\n",
            "1000, 200, 0, cat\n",
            "Epoch:1, Validation Accuracy:61.8, Training Acc: 66.11\n",
            "Epoch:2, Validation Accuracy:61.6, Training Acc: 69.477\n",
            "Epoch:3, Validation Accuracy:60.6, Training Acc: 71.903\n",
            "Epoch:4, Validation Accuracy:60.8, Training Acc: 73.301\n",
            "Epoch:5, Validation Accuracy:61.3, Training Acc: 74.438\n",
            "Epoch:6, Validation Accuracy:59.4, Training Acc: 74.985\n",
            "Epoch:7, Validation Accuracy:59.9, Training Acc: 75.367\n",
            "Epoch:8, Validation Accuracy:60.2, Training Acc: 75.876\n",
            "Epoch:9, Validation Accuracy:60.7, Training Acc: 76.014\n",
            "Epoch:10, Validation Accuracy:59.2, Training Acc: 76.228\n",
            "1000, 200, 0, sum\n",
            "Epoch:1, Validation Accuracy:55.9, Training Acc: 58.538\n",
            "Epoch:2, Validation Accuracy:55.9, Training Acc: 62.137\n",
            "Epoch:3, Validation Accuracy:57.0, Training Acc: 63.931\n",
            "Epoch:4, Validation Accuracy:57.4, Training Acc: 65.042\n",
            "Epoch:5, Validation Accuracy:56.2, Training Acc: 65.552\n",
            "Epoch:6, Validation Accuracy:56.5, Training Acc: 65.863\n",
            "Epoch:7, Validation Accuracy:55.7, Training Acc: 66.411\n",
            "Epoch:8, Validation Accuracy:54.9, Training Acc: 66.604\n",
            "Epoch:9, Validation Accuracy:55.0, Training Acc: 66.89\n",
            "Epoch:10, Validation Accuracy:56.0, Training Acc: 67.216\n",
            "1000, 200, 0, mult\n",
            "Epoch:1, Validation Accuracy:63.2, Training Acc: 71.047\n",
            "Epoch:2, Validation Accuracy:65.5, Training Acc: 85.116\n",
            "Epoch:3, Validation Accuracy:66.2, Training Acc: 92.958\n",
            "Epoch:4, Validation Accuracy:63.9, Training Acc: 96.47\n",
            "Epoch:5, Validation Accuracy:62.9, Training Acc: 98.139\n",
            "Epoch:6, Validation Accuracy:63.9, Training Acc: 98.94\n",
            "Epoch:7, Validation Accuracy:62.7, Training Acc: 99.399\n",
            "Epoch:8, Validation Accuracy:63.5, Training Acc: 99.638\n",
            "Epoch:9, Validation Accuracy:63.2, Training Acc: 99.782\n",
            "Epoch:10, Validation Accuracy:62.8, Training Acc: 99.835\n",
            "1000, 200, 1, cat\n",
            "Epoch:1, Validation Accuracy:62.5, Training Acc: 69.036\n",
            "Epoch:2, Validation Accuracy:66.8, Training Acc: 76.52\n",
            "Epoch:3, Validation Accuracy:66.7, Training Acc: 81.983\n",
            "Epoch:4, Validation Accuracy:66.2, Training Acc: 87.01\n",
            "Epoch:5, Validation Accuracy:65.1, Training Acc: 90.343\n",
            "Epoch:6, Validation Accuracy:64.9, Training Acc: 93.118\n",
            "Epoch:7, Validation Accuracy:63.6, Training Acc: 94.518\n",
            "Epoch:8, Validation Accuracy:64.7, Training Acc: 95.714\n",
            "Epoch:9, Validation Accuracy:64.6, Training Acc: 96.712\n",
            "Epoch:10, Validation Accuracy:65.3, Training Acc: 96.778\n",
            "1000, 200, 1, sum\n",
            "Epoch:1, Validation Accuracy:57.3, Training Acc: 61.372\n",
            "Epoch:2, Validation Accuracy:59.8, Training Acc: 67.311\n",
            "Epoch:3, Validation Accuracy:60.3, Training Acc: 72.52\n",
            "Epoch:4, Validation Accuracy:60.4, Training Acc: 77.098\n",
            "Epoch:5, Validation Accuracy:58.7, Training Acc: 81.114\n",
            "Epoch:6, Validation Accuracy:59.2, Training Acc: 84.2\n",
            "Epoch:7, Validation Accuracy:60.8, Training Acc: 86.794\n",
            "Epoch:8, Validation Accuracy:58.0, Training Acc: 88.755\n",
            "Epoch:9, Validation Accuracy:57.5, Training Acc: 89.473\n",
            "Epoch:10, Validation Accuracy:57.6, Training Acc: 91.316\n",
            "1000, 200, 1, mult\n",
            "Epoch:1, Validation Accuracy:61.2, Training Acc: 69.946\n",
            "Epoch:2, Validation Accuracy:64.6, Training Acc: 83.939\n",
            "Epoch:3, Validation Accuracy:62.3, Training Acc: 91.607\n",
            "Epoch:4, Validation Accuracy:64.0, Training Acc: 95.368\n",
            "Epoch:5, Validation Accuracy:62.6, Training Acc: 97.411\n",
            "Epoch:6, Validation Accuracy:61.4, Training Acc: 98.249\n",
            "Epoch:7, Validation Accuracy:62.9, Training Acc: 98.243\n",
            "Epoch:8, Validation Accuracy:62.4, Training Acc: 99.013\n",
            "Epoch:9, Validation Accuracy:62.7, Training Acc: 99.274\n",
            "Epoch:10, Validation Accuracy:61.6, Training Acc: 99.351\n",
            "5000, 100, 0, cat\n",
            "Epoch:1, Validation Accuracy:61.1, Training Acc: 66.432\n",
            "Epoch:2, Validation Accuracy:61.6, Training Acc: 69.61\n",
            "Epoch:3, Validation Accuracy:62.0, Training Acc: 71.701\n",
            "Epoch:4, Validation Accuracy:61.2, Training Acc: 72.903\n",
            "Epoch:5, Validation Accuracy:60.5, Training Acc: 74.48\n",
            "Epoch:6, Validation Accuracy:60.3, Training Acc: 75.04\n",
            "Epoch:7, Validation Accuracy:60.2, Training Acc: 75.836\n",
            "Epoch:8, Validation Accuracy:61.4, Training Acc: 76.089\n",
            "Epoch:9, Validation Accuracy:61.0, Training Acc: 76.706\n",
            "Epoch:10, Validation Accuracy:60.4, Training Acc: 77.09\n",
            "5000, 100, 0, sum\n",
            "Epoch:1, Validation Accuracy:57.2, Training Acc: 59.424\n",
            "Epoch:2, Validation Accuracy:58.7, Training Acc: 62.551\n",
            "Epoch:3, Validation Accuracy:57.3, Training Acc: 64.546\n",
            "Epoch:4, Validation Accuracy:56.3, Training Acc: 65.498\n",
            "Epoch:5, Validation Accuracy:55.6, Training Acc: 66.091\n",
            "Epoch:6, Validation Accuracy:56.2, Training Acc: 66.536\n",
            "Epoch:7, Validation Accuracy:55.6, Training Acc: 66.774\n",
            "Epoch:8, Validation Accuracy:55.8, Training Acc: 67.036\n",
            "Epoch:9, Validation Accuracy:56.1, Training Acc: 67.092\n",
            "Epoch:10, Validation Accuracy:56.5, Training Acc: 67.193\n",
            "5000, 100, 0, mult\n",
            "Epoch:1, Validation Accuracy:61.2, Training Acc: 68.092\n",
            "Epoch:2, Validation Accuracy:64.3, Training Acc: 80.094\n",
            "Epoch:3, Validation Accuracy:63.4, Training Acc: 87.616\n",
            "Epoch:4, Validation Accuracy:63.6, Training Acc: 92.332\n",
            "Epoch:5, Validation Accuracy:62.7, Training Acc: 94.948\n",
            "Epoch:6, Validation Accuracy:61.4, Training Acc: 96.904\n",
            "Epoch:7, Validation Accuracy:60.4, Training Acc: 97.917\n",
            "Epoch:8, Validation Accuracy:61.4, Training Acc: 98.65\n",
            "Epoch:9, Validation Accuracy:61.2, Training Acc: 99.14\n",
            "Epoch:10, Validation Accuracy:60.4, Training Acc: 99.475\n",
            "5000, 100, 1, cat\n",
            "Epoch:1, Validation Accuracy:64.5, Training Acc: 68.223\n",
            "Epoch:2, Validation Accuracy:64.4, Training Acc: 75.416\n",
            "Epoch:3, Validation Accuracy:65.2, Training Acc: 80.804\n",
            "Epoch:4, Validation Accuracy:66.5, Training Acc: 85.088\n",
            "Epoch:5, Validation Accuracy:63.9, Training Acc: 88.085\n",
            "Epoch:6, Validation Accuracy:65.2, Training Acc: 91.379\n",
            "Epoch:7, Validation Accuracy:64.7, Training Acc: 93.095\n",
            "Epoch:8, Validation Accuracy:64.1, Training Acc: 94.474\n",
            "Epoch:9, Validation Accuracy:64.9, Training Acc: 95.613\n",
            "Epoch:10, Validation Accuracy:64.8, Training Acc: 96.081\n",
            "5000, 100, 1, sum\n",
            "Epoch:1, Validation Accuracy:58.5, Training Acc: 61.171\n",
            "Epoch:2, Validation Accuracy:58.9, Training Acc: 66.622\n",
            "Epoch:3, Validation Accuracy:59.0, Training Acc: 71.372\n",
            "Epoch:4, Validation Accuracy:58.5, Training Acc: 75.519\n",
            "Epoch:5, Validation Accuracy:59.0, Training Acc: 79.299\n",
            "Epoch:6, Validation Accuracy:58.5, Training Acc: 81.882\n",
            "Epoch:7, Validation Accuracy:58.7, Training Acc: 84.123\n",
            "Epoch:8, Validation Accuracy:58.5, Training Acc: 86.204\n",
            "Epoch:9, Validation Accuracy:58.0, Training Acc: 87.452\n",
            "Epoch:10, Validation Accuracy:56.0, Training Acc: 89.03\n",
            "5000, 100, 1, mult\n",
            "Epoch:1, Validation Accuracy:60.6, Training Acc: 68.262\n",
            "Epoch:2, Validation Accuracy:65.5, Training Acc: 79.625\n",
            "Epoch:3, Validation Accuracy:63.5, Training Acc: 87.062\n",
            "Epoch:4, Validation Accuracy:63.7, Training Acc: 91.527\n",
            "Epoch:5, Validation Accuracy:62.7, Training Acc: 94.472\n",
            "Epoch:6, Validation Accuracy:63.3, Training Acc: 95.969\n",
            "Epoch:7, Validation Accuracy:63.5, Training Acc: 96.599\n",
            "Epoch:8, Validation Accuracy:63.4, Training Acc: 97.78\n",
            "Epoch:9, Validation Accuracy:61.7, Training Acc: 98.283\n",
            "Epoch:10, Validation Accuracy:61.9, Training Acc: 98.226\n",
            "5000, 150, 0, cat\n",
            "Epoch:1, Validation Accuracy:62.0, Training Acc: 66.286\n",
            "Epoch:2, Validation Accuracy:61.8, Training Acc: 69.424\n",
            "Epoch:3, Validation Accuracy:61.9, Training Acc: 71.868\n",
            "Epoch:4, Validation Accuracy:61.9, Training Acc: 73.627\n",
            "Epoch:5, Validation Accuracy:62.4, Training Acc: 74.54\n",
            "Epoch:6, Validation Accuracy:60.4, Training Acc: 75.136\n",
            "Epoch:7, Validation Accuracy:61.0, Training Acc: 75.696\n",
            "Epoch:8, Validation Accuracy:59.7, Training Acc: 76.262\n",
            "Epoch:9, Validation Accuracy:61.3, Training Acc: 76.3\n",
            "Epoch:10, Validation Accuracy:60.6, Training Acc: 76.643\n",
            "5000, 150, 0, sum\n",
            "Epoch:1, Validation Accuracy:55.5, Training Acc: 59.34\n",
            "Epoch:2, Validation Accuracy:56.3, Training Acc: 62.839\n",
            "Epoch:3, Validation Accuracy:57.0, Training Acc: 63.579\n",
            "Epoch:4, Validation Accuracy:56.9, Training Acc: 65.118\n",
            "Epoch:5, Validation Accuracy:56.1, Training Acc: 65.788\n",
            "Epoch:6, Validation Accuracy:57.7, Training Acc: 66.101\n",
            "Epoch:7, Validation Accuracy:54.7, Training Acc: 66.771\n",
            "Epoch:8, Validation Accuracy:55.8, Training Acc: 66.761\n",
            "Epoch:9, Validation Accuracy:56.1, Training Acc: 66.987\n",
            "Epoch:10, Validation Accuracy:55.5, Training Acc: 66.89\n",
            "5000, 150, 0, mult\n",
            "Epoch:1, Validation Accuracy:62.8, Training Acc: 68.938\n",
            "Epoch:2, Validation Accuracy:64.7, Training Acc: 83.13\n",
            "Epoch:3, Validation Accuracy:65.3, Training Acc: 91.129\n",
            "Epoch:4, Validation Accuracy:64.3, Training Acc: 95.278\n",
            "Epoch:5, Validation Accuracy:64.1, Training Acc: 97.365\n",
            "Epoch:6, Validation Accuracy:64.7, Training Acc: 98.563\n",
            "Epoch:7, Validation Accuracy:63.8, Training Acc: 99.149\n",
            "Epoch:8, Validation Accuracy:62.9, Training Acc: 99.516\n",
            "Epoch:9, Validation Accuracy:63.6, Training Acc: 99.69\n",
            "Epoch:10, Validation Accuracy:62.9, Training Acc: 99.751\n",
            "5000, 150, 1, cat\n",
            "Epoch:1, Validation Accuracy:63.3, Training Acc: 68.806\n",
            "Epoch:2, Validation Accuracy:65.0, Training Acc: 76.17\n",
            "Epoch:3, Validation Accuracy:65.6, Training Acc: 81.443\n",
            "Epoch:4, Validation Accuracy:65.8, Training Acc: 86.121\n",
            "Epoch:5, Validation Accuracy:66.4, Training Acc: 89.537\n",
            "Epoch:6, Validation Accuracy:66.1, Training Acc: 92.198\n",
            "Epoch:7, Validation Accuracy:66.0, Training Acc: 93.902\n",
            "Epoch:8, Validation Accuracy:64.8, Training Acc: 95.014\n",
            "Epoch:9, Validation Accuracy:65.7, Training Acc: 96.272\n",
            "Epoch:10, Validation Accuracy:64.4, Training Acc: 96.78\n",
            "5000, 150, 1, sum\n",
            "Epoch:1, Validation Accuracy:58.3, Training Acc: 59.973\n",
            "Epoch:2, Validation Accuracy:57.3, Training Acc: 66.593\n",
            "Epoch:3, Validation Accuracy:59.8, Training Acc: 72.258\n",
            "Epoch:4, Validation Accuracy:59.8, Training Acc: 77.214\n",
            "Epoch:5, Validation Accuracy:58.1, Training Acc: 80.913\n",
            "Epoch:6, Validation Accuracy:58.2, Training Acc: 83.838\n",
            "Epoch:7, Validation Accuracy:57.8, Training Acc: 86.086\n",
            "Epoch:8, Validation Accuracy:56.5, Training Acc: 88.107\n",
            "Epoch:9, Validation Accuracy:56.2, Training Acc: 90.06\n",
            "Epoch:10, Validation Accuracy:56.6, Training Acc: 91.043\n",
            "5000, 150, 1, mult\n",
            "Epoch:1, Validation Accuracy:61.3, Training Acc: 69.547\n",
            "Epoch:2, Validation Accuracy:65.1, Training Acc: 81.647\n",
            "Epoch:3, Validation Accuracy:65.3, Training Acc: 90.452\n",
            "Epoch:4, Validation Accuracy:61.9, Training Acc: 94.681\n",
            "Epoch:5, Validation Accuracy:63.3, Training Acc: 96.727\n",
            "Epoch:6, Validation Accuracy:62.7, Training Acc: 97.709\n",
            "Epoch:7, Validation Accuracy:63.5, Training Acc: 98.468\n",
            "Epoch:8, Validation Accuracy:63.4, Training Acc: 98.852\n",
            "Epoch:9, Validation Accuracy:63.4, Training Acc: 99.049\n",
            "Epoch:10, Validation Accuracy:64.2, Training Acc: 99.273\n",
            "5000, 200, 0, cat\n",
            "Epoch:1, Validation Accuracy:61.5, Training Acc: 66.31\n",
            "Epoch:2, Validation Accuracy:63.4, Training Acc: 69.777\n",
            "Epoch:3, Validation Accuracy:62.2, Training Acc: 71.831\n",
            "Epoch:4, Validation Accuracy:59.3, Training Acc: 73.512\n",
            "Epoch:5, Validation Accuracy:59.5, Training Acc: 73.737\n",
            "Epoch:6, Validation Accuracy:60.8, Training Acc: 74.708\n",
            "Epoch:7, Validation Accuracy:60.4, Training Acc: 75.197\n",
            "Epoch:8, Validation Accuracy:61.5, Training Acc: 75.733\n",
            "Epoch:9, Validation Accuracy:59.2, Training Acc: 75.935\n",
            "Epoch:10, Validation Accuracy:59.8, Training Acc: 76.56\n",
            "5000, 200, 0, sum\n",
            "Epoch:1, Validation Accuracy:54.5, Training Acc: 59.291\n",
            "Epoch:2, Validation Accuracy:55.6, Training Acc: 62.391\n",
            "Epoch:3, Validation Accuracy:57.6, Training Acc: 64.016\n",
            "Epoch:4, Validation Accuracy:56.1, Training Acc: 65.236\n",
            "Epoch:5, Validation Accuracy:56.9, Training Acc: 65.655\n",
            "Epoch:6, Validation Accuracy:55.2, Training Acc: 65.946\n",
            "Epoch:7, Validation Accuracy:56.3, Training Acc: 66.628\n",
            "Epoch:8, Validation Accuracy:56.8, Training Acc: 66.525\n",
            "Epoch:9, Validation Accuracy:55.0, Training Acc: 66.681\n",
            "Epoch:10, Validation Accuracy:56.3, Training Acc: 67.05\n",
            "5000, 200, 0, mult\n",
            "Epoch:1, Validation Accuracy:61.9, Training Acc: 70.753\n",
            "Epoch:2, Validation Accuracy:65.0, Training Acc: 84.942\n",
            "Epoch:3, Validation Accuracy:64.7, Training Acc: 92.656\n",
            "Epoch:4, Validation Accuracy:63.7, Training Acc: 96.518\n",
            "Epoch:5, Validation Accuracy:62.4, Training Acc: 98.165\n",
            "Epoch:6, Validation Accuracy:64.1, Training Acc: 99.01\n",
            "Epoch:7, Validation Accuracy:63.4, Training Acc: 99.457\n",
            "Epoch:8, Validation Accuracy:64.1, Training Acc: 99.66\n",
            "Epoch:9, Validation Accuracy:63.9, Training Acc: 99.785\n",
            "Epoch:10, Validation Accuracy:64.4, Training Acc: 99.805\n",
            "5000, 200, 1, cat\n",
            "Epoch:1, Validation Accuracy:63.6, Training Acc: 69.584\n",
            "Epoch:2, Validation Accuracy:67.0, Training Acc: 76.849\n",
            "Epoch:3, Validation Accuracy:67.1, Training Acc: 82.404\n",
            "Epoch:4, Validation Accuracy:65.6, Training Acc: 86.821\n",
            "Epoch:5, Validation Accuracy:67.0, Training Acc: 90.427\n",
            "Epoch:6, Validation Accuracy:64.1, Training Acc: 92.754\n",
            "Epoch:7, Validation Accuracy:64.5, Training Acc: 94.277\n",
            "Epoch:8, Validation Accuracy:63.8, Training Acc: 95.512\n",
            "Epoch:9, Validation Accuracy:63.5, Training Acc: 96.673\n",
            "Epoch:10, Validation Accuracy:65.0, Training Acc: 96.89\n",
            "5000, 200, 1, sum\n",
            "Epoch:1, Validation Accuracy:56.9, Training Acc: 62.552\n",
            "Epoch:2, Validation Accuracy:59.4, Training Acc: 67.919\n",
            "Epoch:3, Validation Accuracy:59.4, Training Acc: 73.176\n",
            "Epoch:4, Validation Accuracy:58.9, Training Acc: 77.105\n",
            "Epoch:5, Validation Accuracy:58.3, Training Acc: 80.806\n",
            "Epoch:6, Validation Accuracy:58.9, Training Acc: 83.421\n",
            "Epoch:7, Validation Accuracy:59.3, Training Acc: 85.912\n",
            "Epoch:8, Validation Accuracy:57.0, Training Acc: 88.02\n",
            "Epoch:9, Validation Accuracy:57.1, Training Acc: 90.079\n",
            "Epoch:10, Validation Accuracy:57.3, Training Acc: 91.5\n",
            "5000, 200, 1, mult\n",
            "Epoch:1, Validation Accuracy:62.1, Training Acc: 69.98\n",
            "Epoch:2, Validation Accuracy:63.6, Training Acc: 83.884\n",
            "Epoch:3, Validation Accuracy:64.6, Training Acc: 91.574\n",
            "Epoch:4, Validation Accuracy:64.3, Training Acc: 95.309\n",
            "Epoch:5, Validation Accuracy:62.8, Training Acc: 97.204\n",
            "Epoch:6, Validation Accuracy:61.7, Training Acc: 97.863\n",
            "Epoch:7, Validation Accuracy:62.4, Training Acc: 98.77\n",
            "Epoch:8, Validation Accuracy:63.3, Training Acc: 99.096\n",
            "Epoch:9, Validation Accuracy:61.8, Training Acc: 99.136\n",
            "Epoch:10, Validation Accuracy:62.0, Training Acc: 99.476\n",
            "10000, 100, 0, cat\n",
            "Epoch:1, Validation Accuracy:61.1, Training Acc: 66.219\n",
            "Epoch:2, Validation Accuracy:62.2, Training Acc: 69.501\n",
            "Epoch:3, Validation Accuracy:60.6, Training Acc: 71.706\n",
            "Epoch:4, Validation Accuracy:62.9, Training Acc: 73.128\n",
            "Epoch:5, Validation Accuracy:61.2, Training Acc: 74.249\n",
            "Epoch:6, Validation Accuracy:61.0, Training Acc: 75.084\n",
            "Epoch:7, Validation Accuracy:60.6, Training Acc: 75.966\n",
            "Epoch:8, Validation Accuracy:59.2, Training Acc: 76.318\n",
            "Epoch:9, Validation Accuracy:59.0, Training Acc: 76.797\n",
            "Epoch:10, Validation Accuracy:61.0, Training Acc: 76.866\n",
            "10000, 100, 0, sum\n",
            "Epoch:1, Validation Accuracy:56.9, Training Acc: 59.196\n",
            "Epoch:2, Validation Accuracy:57.7, Training Acc: 62.917\n",
            "Epoch:3, Validation Accuracy:57.0, Training Acc: 64.567\n",
            "Epoch:4, Validation Accuracy:56.7, Training Acc: 65.525\n",
            "Epoch:5, Validation Accuracy:55.8, Training Acc: 66.01\n",
            "Epoch:6, Validation Accuracy:56.3, Training Acc: 66.593\n",
            "Epoch:7, Validation Accuracy:54.7, Training Acc: 66.764\n",
            "Epoch:8, Validation Accuracy:55.3, Training Acc: 67.097\n",
            "Epoch:9, Validation Accuracy:56.2, Training Acc: 67.152\n",
            "Epoch:10, Validation Accuracy:54.3, Training Acc: 66.965\n",
            "10000, 100, 0, mult\n",
            "Epoch:1, Validation Accuracy:62.7, Training Acc: 68.553\n",
            "Epoch:2, Validation Accuracy:62.4, Training Acc: 80.106\n",
            "Epoch:3, Validation Accuracy:62.7, Training Acc: 87.464\n",
            "Epoch:4, Validation Accuracy:63.6, Training Acc: 91.939\n",
            "Epoch:5, Validation Accuracy:63.4, Training Acc: 94.935\n",
            "Epoch:6, Validation Accuracy:63.5, Training Acc: 96.757\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLUI5Kg9ofVh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bd3a7244-30be-4c75-c68b-23eb7fd6e0c0"
      },
      "source": [
        "from itertools import product\n",
        "\n",
        "vocab_size_list = [1000, 5000, 10000]\n",
        "emb_dim_list = [100, 150, 200]\n",
        "nn_ind_list = [0, 1]\n",
        "interaction_list = ['cat','sum','mult']\n",
        "results_dict = {}\n",
        "\n",
        "a = [vocab_size_list, emb_dim_list, nn_ind_list]\n",
        "a"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1000, 5000, 10000], [100, 150, 200], [0, 1]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJWb3tCAs4NT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}