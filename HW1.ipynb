{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZKMekmNeAOd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnTmnbFMeClq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "2ef3e061-0c74-43b3-f062-21e99a53a204"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TCB_YQLeDbR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ed63dc11-f164-4a05-ce06-4cfe9999c6cd"
      },
      "source": [
        "# Let's write the tokenization function \n",
        "\n",
        "import spacy\n",
        "import string\n",
        "import pickle as pkl\n",
        "import torch\n",
        "\n",
        "# Load English tokenizer, tagger, parser, NER and word vectors\n",
        "tokenizer = spacy.load('en_core_web_sm')\n",
        "punctuations = string.punctuation\n",
        "\n",
        "# lowercase and remove punctuation\n",
        "def tokenize(sent):\n",
        "    tokens = tokenizer(sent)\n",
        "    return [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
        "\n",
        "# Example\n",
        "tokens = tokenize(u'Apple is looking at buying U.K. startup for $1 billion.')\n",
        "print (tokens)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['apple', 'is', 'looking', 'at', 'buying', 'u.k.', 'startup', 'for', '1', 'billion']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlDPVn9_j83N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is the code cell that tokenizes train/val/test datasets\n",
        "# However it takes about 15-20 minutes to run it\n",
        "# For convinience we have provided the preprocessed datasets\n",
        "# Please see the next code cell\n",
        "\n",
        "\n",
        "def tokenize_dataset(dataset):\n",
        "    sentence1_tokens_dataset = []\n",
        "    sentence2_tokens_dataset = []\n",
        "    # we are keeping track of all tokens in dataset \n",
        "    # in order to create vocabulary later\n",
        "    all_tokens = []\n",
        "    label_list = []\n",
        "    for sample in dataset:\n",
        "        sentence1, sentence2, label = sample.split(\"\\t\")\n",
        "        if label[-1] == '\\n':\n",
        "            label = label[:-1]\n",
        "        sentence1_tokens = tokenize(sentence1)\n",
        "        sentence2_tokens = tokenize(sentence2)\n",
        "        sentence1_tokens_dataset.append(sentence1_tokens)\n",
        "        sentence2_tokens_dataset.append(sentence2_tokens)\n",
        "        label_list.append(label)\n",
        "        all_tokens += sentence1_tokens\n",
        "        all_tokens += sentence2_tokens\n",
        "\n",
        "    return sentence1_tokens_dataset, sentence2_tokens_dataset, label_list, all_tokens\n",
        "\n",
        "#val set tokens\n",
        "print (\"Tokenizing val data\")\n",
        "val_data = open(\"/content/drive/My Drive/NLP_HW/HW1/snli_val.tsv\",\"r\")\n",
        "sentence1_val_tokens_dataset, sentence2_val_tokens_dataset, snli_val_label_list, _ = tokenize_dataset(val_data)\n",
        "pkl.dump(sentence1_val_tokens_dataset, open(\"/content/drive/My Drive/NLP_HW/HW1/snli_val_sentence1_tokens.p\", \"wb\"))\n",
        "pkl.dump(sentence2_val_tokens_dataset, open(\"/content/drive/My Drive/NLP_HW/HW1/snli_val_sentence2_tokens.p\", \"wb\"))\n",
        "pkl.dump(snli_val_label_list, open(\"/content/drive/My Drive/NLP_HW/HW1/snli_val_label_list.p\", \"wb\"))\n",
        "\n",
        "#train set tokens\n",
        "print (\"Tokenizing train data\")\n",
        "train_data = open(\"/content/drive/My Drive/NLP_HW/HW1/snli_train.tsv\",\"r\")\n",
        "sentence1_train_tokens_dataset, sentence2_train_tokens_dataset, snli_train_label_list, snli_train_tokens = tokenize_dataset(train_data)\n",
        "pkl.dump(sentence1_train_tokens_dataset, open(\"/content/drive/My Drive/NLP_HW/HW1/snli_train_sentence1_tokens.p\", \"wb\"))\n",
        "pkl.dump(sentence2_train_tokens_dataset, open(\"/content/drive/My Drive/NLP_HW/HW1/snli_train_sentence2_tokens.p\", \"wb\"))\n",
        "pkl.dump(snli_train_label_list, open(\"/content/drive/My Drive/NLP_HW/HW1/snli_train_label_list.p\", \"wb\"))\n",
        "pkl.dump(snli_train_tokens, open(\"/content/drive/My Drive/NLP_HW/HW1/snli_train_tokens.p\", \"wb\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wVSxbwpYX8t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence1_train_tokens_dataset = pkl.load(open(\"/content/drive/My Drive/NLP_HW/HW1/snli_train_sentence1_tokens.p\", \"rb\"))\n",
        "sentence2_train_tokens_dataset = pkl.load(open(\"/content/drive/My Drive/NLP_HW/HW1/snli_train_sentence2_tokens.p\", \"rb\"))\n",
        "snli_train_label_list = pkl.load(open(\"/content/drive/My Drive/NLP_HW/HW1/snli_train_label_list.p\", \"rb\"))\n",
        "snli_train_tokens = pkl.load(open(\"/content/drive/My Drive/NLP_HW/HW1/snli_train_tokens.p\", \"rb\"))\n",
        "\n",
        "sentence1_val_tokens_dataset = pkl.load(open(\"/content/drive/My Drive/NLP_HW/HW1/snli_val_sentence1_tokens.p\", \"rb\"))\n",
        "sentence2_val_tokens_dataset = pkl.load(open(\"/content/drive/My Drive/NLP_HW/HW1/snli_val_sentence2_tokens.p\", \"rb\"))\n",
        "snli_val_label_list = pkl.load(open(\"/content/drive/My Drive/NLP_HW/HW1/snli_val_label_list.p\", \"rb\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdCY-uhm9Dms",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_target = []\n",
        "val_target = []\n",
        "target_dict = {'contradiction':0, 'entailment':1, 'neutral':2}\n",
        "for target in snli_train_label_list[1:]:\n",
        "    train_target.append(target_dict[target])\n",
        "for target in snli_val_label_list[1:]:\n",
        "    val_target.append(target_dict[target])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rgy4n38BpGjZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_SENTENCE_LENGTH = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuPZwyp_pGsu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SNLIDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
        "    Note that this class inherits torch.utils.data.Dataset\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, sentence1_list, sentence2_list, target_list):\n",
        "        \"\"\"\n",
        "        @param data_list: list of newsgroup tokens \n",
        "        @param target_list: list of newsgroup targets \n",
        "\n",
        "        \"\"\"\n",
        "        self.sentence1_list = sentence1_list\n",
        "        self.sentence2_list = sentence2_list\n",
        "        self.target_list = target_list\n",
        "        assert (len(self.sentence1_list) == len(self.target_list))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.target_list)\n",
        "        \n",
        "    def __getitem__(self, key):\n",
        "        \"\"\"\n",
        "        Triggered when you call dataset[i]\n",
        "        \"\"\"\n",
        "        \n",
        "        token1_idx = self.sentence1_list[key][:MAX_SENTENCE_LENGTH]\n",
        "        token2_idx = self.sentence2_list[key][:MAX_SENTENCE_LENGTH]\n",
        "        label = self.target_list[key]\n",
        "        return [token1_idx, token2_idx, len(token1_idx), len(token2_idx), label]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKhaVJj3kb2J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "# save index 0 for unk and 1 for pad\n",
        "PAD_IDX = 0\n",
        "UNK_IDX = 1\n",
        "\n",
        "def build_vocab(all_tokens, max_vocab_size=10**4):\n",
        "    # Returns:\n",
        "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
        "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
        "    token_counter = Counter(all_tokens)\n",
        "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
        "    id2token = list(vocab)\n",
        "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
        "    id2token = ['<pad>', '<unk>'] + id2token\n",
        "    token2id['<pad>'] = PAD_IDX \n",
        "    token2id['<unk>'] = UNK_IDX\n",
        "    return token2id, id2token\n",
        "\n",
        "token2id, id2token = build_vocab(snli_train_tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bn1DaZXHkaeR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "e6afaaef-80f7-4a9c-e23a-0ad06c595cda"
      },
      "source": [
        "# Lets check the dictionary by loading random token from it\n",
        "import random\n",
        "\n",
        "random_token_id = random.randint(0, len(id2token)-1)\n",
        "random_token = id2token[random_token_id]\n",
        "\n",
        "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
        "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token id 9845 ; token picasso\n",
            "Token picasso; token id 9845\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKzeSf6Vkjp8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "8f830b4f-5ff1-4381-c4c0-40ee1dc410d5"
      },
      "source": [
        "# convert token to id in the dataset\n",
        "def token2index_dataset(tokens_data):\n",
        "    indices_data = []\n",
        "    for tokens in tokens_data:\n",
        "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
        "        indices_data.append(index_list)\n",
        "    return indices_data\n",
        "\n",
        "sent1_train_indices = token2index_dataset(sentence1_train_tokens_dataset)\n",
        "sent2_train_indices = token2index_dataset(sentence2_train_tokens_dataset)\n",
        "sent1_val_indices = token2index_dataset(sentence1_val_tokens_dataset)\n",
        "sent2_val_indices = token2index_dataset(sentence2_val_tokens_dataset)\n",
        "\n",
        "# double checking\n",
        "print (\"Train dataset size is {}\".format(len(sent1_train_indices)))\n",
        "print (\"Train dataset size is {}\".format(len(sentence1_train_tokens_dataset)))\n",
        "print (\"Train dataset size is {}\".format(len(sent2_train_indices)))\n",
        "print (\"Train dataset size is {}\".format(len(sentence2_train_tokens_dataset)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train dataset size is 100001\n",
            "Train dataset size is 100001\n",
            "Train dataset size is 100001\n",
            "Train dataset size is 100001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lqtoe8DbpRQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = SNLIDataset(sent1_train_indices[1:], sent2_train_indices[1:], train_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3yK-WnXqLNY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def snli_collate_func(batch):\n",
        "    \"\"\"\n",
        "    Customized function for DataLoader that dynamically pads the batch so that all \n",
        "    data have the same length\n",
        "    \"\"\"\n",
        "    sent1_list = []\n",
        "    sent2_list = []\n",
        "    label_list = []\n",
        "    length_list_sent1 = []\n",
        "    length_list_sent2 = []\n",
        "    #print(\"collate batch: \", batch[0][0])\n",
        "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
        "    for datum in batch:\n",
        "        label_list.append(datum[4])\n",
        "        length_list_sent1.append(datum[2])\n",
        "        length_list_sent2.append(datum[3])\n",
        "    # padding\n",
        "    for datum in batch:\n",
        "        padded_vec1 = np.pad(np.array(datum[0]), \n",
        "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[2])), \n",
        "                                mode=\"constant\", constant_values=0)\n",
        "        sent1_list.append(padded_vec1)\n",
        "        padded_vec2 = np.pad(np.array(datum[1]), \n",
        "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[3])), \n",
        "                                mode=\"constant\", constant_values=0)\n",
        "        sent2_list.append(padded_vec2)\n",
        "    return [torch.from_numpy(np.array(sent1_list)), torch.from_numpy(np.array(sent2_list)), torch.LongTensor(length_list_sent1),torch.LongTensor(length_list_sent2), torch.LongTensor(label_list)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNt9PZYqsGt5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 256\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           collate_fn=snli_collate_func,\n",
        "                                           shuffle=True)\n",
        "\n",
        "val_dataset = SNLIDataset(sent1_val_indices[1:], sent2_val_indices[1:], val_target)\n",
        "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           collate_fn=snli_collate_func,\n",
        "                                           shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuBtdxsggR4J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First import torch related libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BagOfWords_Model(nn.Module):\n",
        "    \"\"\"\n",
        "    BagOfWords classification model\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, emb_dim, nn_ind = False, interaction_type = 'cat'):\n",
        "        \"\"\"\n",
        "        @param vocab_size: size of the vocabulary. \n",
        "        @param emb_dim: size of the word embedding\n",
        "        \"\"\"\n",
        "        super(BagOfWords_Model, self).__init__()\n",
        "        # pay attention to padding_idx \n",
        "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
        "        self.nn_ind = nn_ind\n",
        "        self.interaction_type = interaction_type\n",
        "        if self.interaction_type =='cat':\n",
        "            if nn_ind:\n",
        "                self.linear1 = nn.Linear(2*emb_dim, emb_dim)\n",
        "                self.relu1 = nn.ReLU()\n",
        "                self.linear2 = nn.Linear(emb_dim, int(emb_dim/2))\n",
        "                self.relu2 = nn.ReLU()\n",
        "                self.linear3 = nn.Linear(int(emb_dim/2), 3)\n",
        "            else:\n",
        "                self.linear = nn.Linear(2*emb_dim,3)\n",
        "        else:\n",
        "            if nn_ind:\n",
        "                self.linear1 = nn.Linear(emb_dim, int(emb_dim/2))\n",
        "                self.relu1 = nn.ReLU()\n",
        "                self.linear2 = nn.Linear(int(emb_dim/2),int(emb_dim/4))\n",
        "                self.relu2 = nn.ReLU()\n",
        "                self.linear3 = nn.Linear(int(emb_dim/4), 3)\n",
        "            else:\n",
        "                self.linear = nn.Linear(emb_dim,3)\n",
        "    \n",
        "    def interaction_func(self, sent1, sent2):\n",
        "        if self.interaction_type == 'sum':\n",
        "            out = sent1.float() + sent2.float()\n",
        "        elif self.interaction_type == 'cat':\n",
        "            out = torch.cat((sent1.float(),sent2.float()),dim=1)\n",
        "        else:\n",
        "            out = sent1.float()*sent2.float()\n",
        "        return out\n",
        "    \n",
        "    def forward(self, sent1, sent2, length1, length2):\n",
        "        \"\"\"\n",
        "        \n",
        "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
        "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
        "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
        "            length of each sentences in the data.\n",
        "        \"\"\"\n",
        "        out1 = self.embed(sent1)\n",
        "        out1 = torch.sum(out1, dim=1)\n",
        "        out1 /= length1.view(length1.size()[0],1).expand_as(out1).float()\n",
        "        out2 = self.embed(sent2)\n",
        "        out2 = torch.sum(out2, dim=1)\n",
        "        out2 /= length2.view(length2.size()[0],1).expand_as(out2).float()\n",
        "        out = self.interaction_func(out1, out2)\n",
        "\n",
        "        # return logits\n",
        "        \n",
        "        if self.nn_ind:\n",
        "            out = self.relu1(self.linear1(out))\n",
        "            out = self.relu2(self.linear2(out))\n",
        "            out = self.linear3(out)            \n",
        "        else:\n",
        "            out = self.linear(out)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35k7xEMg30r7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 10 # number epoch to train\n",
        "\n",
        "# Function for testing the model\n",
        "def test_model(loader, model):\n",
        "    \"\"\"\n",
        "    Help function that tests the model's performance on a dataset\n",
        "    @param: loader - data loader for the dataset to test against\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for sent1, sent2, length1, length2, labels in loader:\n",
        "            sent1_batch, sent2_batch, length1_batch, length2_batch, label_batch = sent1.to(device), sent2.to(device), length1.to(device), length2.to(device), labels.to(device)\n",
        "            outputs = F.softmax(model(sent1_batch, sent2_batch, length1_batch, length2_batch), dim=1).to(device)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            total += label_batch.size(0)\n",
        "            correct += predicted.eq(label_batch.view_as(predicted)).sum().item()\n",
        "    return (100 * correct / total)\n",
        "\n",
        "def train_model(train_loader, val_loader, model, optimizer, criterion, num_epochs):\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = 0\n",
        "        total = 0\n",
        "        i = 0\n",
        "        for sent1, sent2, length1, length2, labels in train_loader:\n",
        "            model.train()\n",
        "            sent1_batch, sent2_batch, length1_batch, length2_batch, label_batch = sent1.to(device), sent2.to(device), length1.to(device), length2.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(sent1_batch, sent2_batch, length1_batch, length2_batch)\n",
        "            loss = criterion(outputs, label_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "            total += label_batch.size(0)\n",
        "            i += 1\n",
        "            # validate every 100 iterations\n",
        "            if i > 0 and i % 100 == 0:\n",
        "                # validate\n",
        "                val_acc = test_model(val_loader, model)\n",
        "                print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
        "        train_acc = test_model(train_loader, model)\n",
        "        print(\"Epoch:{}, Training Loss:{}, Training Acc: {}\".format(epoch+1, train_loss/total, train_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_KZoGXmSrdN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "outputId": "802dd3e1-ccbf-470f-cd91-562e3908974d"
      },
      "source": [
        "emb_dim = 100\n",
        "model = BagOfWords_Model(len(id2token), emb_dim).to(device)\n",
        "criterion = torch.nn.CrossEntropyLoss(reduction='sum')  \n",
        "\n",
        "learning_rate = 0.01\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "train_model(train_loader, val_loader, model, optimizer, criterion, num_epochs)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: [1/10], Step: [101/391], Validation Acc: 57.3\n",
            "Epoch: [1/10], Step: [201/391], Validation Acc: 62.1\n",
            "Epoch: [1/10], Step: [301/391], Validation Acc: 62.3\n",
            "Epoch:1, Training Loss:0.895773776550293, Training Acc: 65.149\n",
            "Epoch: [2/10], Step: [101/391], Validation Acc: 61.7\n",
            "Epoch: [2/10], Step: [201/391], Validation Acc: 63.9\n",
            "Epoch: [2/10], Step: [301/391], Validation Acc: 63.0\n",
            "Epoch:2, Training Loss:0.8010885529327393, Training Acc: 68.078\n",
            "Epoch: [3/10], Step: [101/391], Validation Acc: 62.3\n",
            "Epoch: [3/10], Step: [201/391], Validation Acc: 62.6\n",
            "Epoch: [3/10], Step: [301/391], Validation Acc: 61.9\n",
            "Epoch:3, Training Loss:0.7670704032897949, Training Acc: 69.76\n",
            "Epoch: [4/10], Step: [101/391], Validation Acc: 60.8\n",
            "Epoch: [4/10], Step: [201/391], Validation Acc: 63.3\n",
            "Epoch: [4/10], Step: [301/391], Validation Acc: 61.6\n",
            "Epoch:4, Training Loss:0.7428371154785156, Training Acc: 70.912\n",
            "Epoch: [5/10], Step: [101/391], Validation Acc: 62.2\n",
            "Epoch: [5/10], Step: [201/391], Validation Acc: 61.1\n",
            "Epoch: [5/10], Step: [301/391], Validation Acc: 62.9\n",
            "Epoch:5, Training Loss:0.7247338473510743, Training Acc: 71.607\n",
            "Epoch: [6/10], Step: [101/391], Validation Acc: 61.3\n",
            "Epoch: [6/10], Step: [201/391], Validation Acc: 61.4\n",
            "Epoch: [6/10], Step: [301/391], Validation Acc: 61.4\n",
            "Epoch:6, Training Loss:0.7113110131835938, Training Acc: 72.136\n",
            "Epoch: [7/10], Step: [101/391], Validation Acc: 60.5\n",
            "Epoch: [7/10], Step: [201/391], Validation Acc: 61.6\n",
            "Epoch: [7/10], Step: [301/391], Validation Acc: 60.8\n",
            "Epoch:7, Training Loss:0.7002135668945313, Training Acc: 72.477\n",
            "Epoch: [8/10], Step: [101/391], Validation Acc: 61.4\n",
            "Epoch: [8/10], Step: [201/391], Validation Acc: 60.4\n",
            "Epoch: [8/10], Step: [301/391], Validation Acc: 62.1\n",
            "Epoch:8, Training Loss:0.6917602828216552, Training Acc: 73.26\n",
            "Epoch: [9/10], Step: [101/391], Validation Acc: 61.6\n",
            "Epoch: [9/10], Step: [201/391], Validation Acc: 61.4\n",
            "Epoch: [9/10], Step: [301/391], Validation Acc: 60.5\n",
            "Epoch:9, Training Loss:0.6847589283752441, Training Acc: 73.257\n",
            "Epoch: [10/10], Step: [101/391], Validation Acc: 61.1\n",
            "Epoch: [10/10], Step: [201/391], Validation Acc: 60.3\n",
            "Epoch: [10/10], Step: [301/391], Validation Acc: 61.8\n",
            "Epoch:10, Training Loss:0.6796847344207764, Training Acc: 73.31\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHHvMt3if9Fy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "outputId": "1b3df7cb-e02c-4842-d49a-ba9de37d2e25"
      },
      "source": [
        "emb_dim = 100\n",
        "model = BagOfWords_Model(len(id2token), emb_dim, interaction_type = 'sum').to(device)\n",
        "criterion = torch.nn.CrossEntropyLoss(reduction='sum')  \n",
        "\n",
        "learning_rate = 0.01\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "train_model(train_loader, val_loader, model, optimizer, criterion, num_epochs)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: [1/10], Step: [101/391], Validation Acc: 50.3\n",
            "Epoch: [1/10], Step: [201/391], Validation Acc: 53.3\n",
            "Epoch: [1/10], Step: [301/391], Validation Acc: 55.5\n",
            "Epoch:1, Training Loss:0.9881934674072266, Training Acc: 58.524\n",
            "Epoch: [2/10], Step: [101/391], Validation Acc: 56.7\n",
            "Epoch: [2/10], Step: [201/391], Validation Acc: 56.4\n",
            "Epoch: [2/10], Step: [301/391], Validation Acc: 56.5\n",
            "Epoch:2, Training Loss:0.9124954850769043, Training Acc: 60.629\n",
            "Epoch: [3/10], Step: [101/391], Validation Acc: 55.9\n",
            "Epoch: [3/10], Step: [201/391], Validation Acc: 56.8\n",
            "Epoch: [3/10], Step: [301/391], Validation Acc: 56.4\n",
            "Epoch:3, Training Loss:0.8872130285644532, Training Acc: 62.189\n",
            "Epoch: [4/10], Step: [101/391], Validation Acc: 57.2\n",
            "Epoch: [4/10], Step: [201/391], Validation Acc: 56.7\n",
            "Epoch: [4/10], Step: [301/391], Validation Acc: 58.2\n",
            "Epoch:4, Training Loss:0.8726772467041015, Training Acc: 62.598\n",
            "Epoch: [5/10], Step: [101/391], Validation Acc: 57.1\n",
            "Epoch: [5/10], Step: [201/391], Validation Acc: 55.8\n",
            "Epoch: [5/10], Step: [301/391], Validation Acc: 56.5\n",
            "Epoch:5, Training Loss:0.8628876374816895, Training Acc: 62.997\n",
            "Epoch: [6/10], Step: [101/391], Validation Acc: 57.2\n",
            "Epoch: [6/10], Step: [201/391], Validation Acc: 56.8\n",
            "Epoch: [6/10], Step: [301/391], Validation Acc: 57.6\n",
            "Epoch:6, Training Loss:0.8560897412109375, Training Acc: 63.322\n",
            "Epoch: [7/10], Step: [101/391], Validation Acc: 57.4\n",
            "Epoch: [7/10], Step: [201/391], Validation Acc: 57.4\n",
            "Epoch: [7/10], Step: [301/391], Validation Acc: 57.9\n",
            "Epoch:7, Training Loss:0.8505946795654297, Training Acc: 63.273\n",
            "Epoch: [8/10], Step: [101/391], Validation Acc: 56.3\n",
            "Epoch: [8/10], Step: [201/391], Validation Acc: 57.8\n",
            "Epoch: [8/10], Step: [301/391], Validation Acc: 57.6\n",
            "Epoch:8, Training Loss:0.8476061441802979, Training Acc: 63.483\n",
            "Epoch: [9/10], Step: [101/391], Validation Acc: 56.0\n",
            "Epoch: [9/10], Step: [201/391], Validation Acc: 55.5\n",
            "Epoch: [9/10], Step: [301/391], Validation Acc: 57.6\n",
            "Epoch:9, Training Loss:0.8436969682312012, Training Acc: 63.41\n",
            "Epoch: [10/10], Step: [101/391], Validation Acc: 57.5\n",
            "Epoch: [10/10], Step: [201/391], Validation Acc: 56.2\n",
            "Epoch: [10/10], Step: [301/391], Validation Acc: 57.8\n",
            "Epoch:10, Training Loss:0.8406557514953613, Training Acc: 63.742\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iD9Itx-fgGQS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "outputId": "8ac6d1d2-eae2-464c-d578-91432c421827"
      },
      "source": [
        "emb_dim = 100\n",
        "model = BagOfWords_Model(len(id2token), emb_dim, interaction_type = 'mult').to(device)\n",
        "criterion = torch.nn.CrossEntropyLoss(reduction='sum')  \n",
        "\n",
        "learning_rate = 0.01\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "train_model(train_loader, val_loader, model, optimizer, criterion, num_epochs)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: [1/10], Step: [101/391], Validation Acc: 48.6\n",
            "Epoch: [1/10], Step: [201/391], Validation Acc: 56.4\n",
            "Epoch: [1/10], Step: [301/391], Validation Acc: 60.5\n",
            "Epoch:1, Training Loss:0.9477548596191406, Training Acc: 67.863\n",
            "Epoch: [2/10], Step: [101/391], Validation Acc: 62.2\n",
            "Epoch: [2/10], Step: [201/391], Validation Acc: 62.6\n",
            "Epoch: [2/10], Step: [301/391], Validation Acc: 63.4\n",
            "Epoch:2, Training Loss:0.7266880146026611, Training Acc: 78.683\n",
            "Epoch: [3/10], Step: [101/391], Validation Acc: 62.9\n",
            "Epoch: [3/10], Step: [201/391], Validation Acc: 63.7\n",
            "Epoch: [3/10], Step: [301/391], Validation Acc: 63.7\n",
            "Epoch:3, Training Loss:0.5479757135009765, Training Acc: 86.563\n",
            "Epoch: [4/10], Step: [101/391], Validation Acc: 64.0\n",
            "Epoch: [4/10], Step: [201/391], Validation Acc: 65.1\n",
            "Epoch: [4/10], Step: [301/391], Validation Acc: 64.6\n",
            "Epoch:4, Training Loss:0.3973693204498291, Training Acc: 91.264\n",
            "Epoch: [5/10], Step: [101/391], Validation Acc: 64.4\n",
            "Epoch: [5/10], Step: [201/391], Validation Acc: 64.2\n",
            "Epoch: [5/10], Step: [301/391], Validation Acc: 63.5\n",
            "Epoch:5, Training Loss:0.2835773520278931, Training Acc: 94.432\n",
            "Epoch: [6/10], Step: [101/391], Validation Acc: 63.1\n",
            "Epoch: [6/10], Step: [201/391], Validation Acc: 63.2\n",
            "Epoch: [6/10], Step: [301/391], Validation Acc: 64.2\n",
            "Epoch:6, Training Loss:0.20250867618560792, Training Acc: 96.479\n",
            "Epoch: [7/10], Step: [101/391], Validation Acc: 64.4\n",
            "Epoch: [7/10], Step: [201/391], Validation Acc: 63.3\n",
            "Epoch: [7/10], Step: [301/391], Validation Acc: 62.9\n",
            "Epoch:7, Training Loss:0.14351832063674927, Training Acc: 97.637\n",
            "Epoch: [8/10], Step: [101/391], Validation Acc: 62.9\n",
            "Epoch: [8/10], Step: [201/391], Validation Acc: 62.3\n",
            "Epoch: [8/10], Step: [301/391], Validation Acc: 63.1\n",
            "Epoch:8, Training Loss:0.10321155923843384, Training Acc: 98.465\n",
            "Epoch: [9/10], Step: [101/391], Validation Acc: 63.4\n",
            "Epoch: [9/10], Step: [201/391], Validation Acc: 62.1\n",
            "Epoch: [9/10], Step: [301/391], Validation Acc: 62.5\n",
            "Epoch:9, Training Loss:0.07361089518547058, Training Acc: 98.958\n",
            "Epoch: [10/10], Step: [101/391], Validation Acc: 62.0\n",
            "Epoch: [10/10], Step: [201/391], Validation Acc: 61.7\n",
            "Epoch: [10/10], Step: [301/391], Validation Acc: 62.6\n",
            "Epoch:10, Training Loss:0.0535922446680069, Training Acc: 99.339\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBPY8_ld8vzh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "outputId": "847e8ede-e387-4807-8286-a39568bcd9d5"
      },
      "source": [
        "emb_dim = 100\n",
        "model1 = BagOfWords_Model(len(id2token), emb_dim, nn_ind=True).to(device)\n",
        "criterion1 = torch.nn.CrossEntropyLoss(reduction='sum')  \n",
        "\n",
        "learning_rate = 0.01\n",
        "optimizer1 = torch.optim.Adam(model1.parameters(), lr=learning_rate)\n",
        "train_model(train_loader, val_loader, model1, optimizer1, criterion1, num_epochs)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: [1/10], Step: [101/391], Validation Acc: 57.8\n",
            "Epoch: [1/10], Step: [201/391], Validation Acc: 60.8\n",
            "Epoch: [1/10], Step: [301/391], Validation Acc: 62.5\n",
            "Epoch:1, Training Loss:0.8840453399658204, Training Acc: 68.085\n",
            "Epoch: [2/10], Step: [101/391], Validation Acc: 64.3\n",
            "Epoch: [2/10], Step: [201/391], Validation Acc: 65.5\n",
            "Epoch: [2/10], Step: [301/391], Validation Acc: 63.7\n",
            "Epoch:2, Training Loss:0.7363392852020264, Training Acc: 74.715\n",
            "Epoch: [3/10], Step: [101/391], Validation Acc: 65.8\n",
            "Epoch: [3/10], Step: [201/391], Validation Acc: 66.3\n",
            "Epoch: [3/10], Step: [301/391], Validation Acc: 67.1\n",
            "Epoch:3, Training Loss:0.6400900885009766, Training Acc: 79.246\n",
            "Epoch: [4/10], Step: [101/391], Validation Acc: 64.4\n",
            "Epoch: [4/10], Step: [201/391], Validation Acc: 66.1\n",
            "Epoch: [4/10], Step: [301/391], Validation Acc: 66.7\n",
            "Epoch:4, Training Loss:0.5474358529663086, Training Acc: 82.701\n",
            "Epoch: [5/10], Step: [101/391], Validation Acc: 66.2\n",
            "Epoch: [5/10], Step: [201/391], Validation Acc: 66.1\n",
            "Epoch: [5/10], Step: [301/391], Validation Acc: 65.6\n",
            "Epoch:5, Training Loss:0.45928359977722166, Training Acc: 86.591\n",
            "Epoch: [6/10], Step: [101/391], Validation Acc: 65.9\n",
            "Epoch: [6/10], Step: [201/391], Validation Acc: 64.4\n",
            "Epoch: [6/10], Step: [301/391], Validation Acc: 65.2\n",
            "Epoch:6, Training Loss:0.3778921021270752, Training Acc: 89.514\n",
            "Epoch: [7/10], Step: [101/391], Validation Acc: 64.4\n",
            "Epoch: [7/10], Step: [201/391], Validation Acc: 64.2\n",
            "Epoch: [7/10], Step: [301/391], Validation Acc: 65.2\n",
            "Epoch:7, Training Loss:0.3140995894241333, Training Acc: 91.491\n",
            "Epoch: [8/10], Step: [101/391], Validation Acc: 65.0\n",
            "Epoch: [8/10], Step: [201/391], Validation Acc: 64.6\n",
            "Epoch: [8/10], Step: [301/391], Validation Acc: 64.6\n",
            "Epoch:8, Training Loss:0.25730897926330565, Training Acc: 92.792\n",
            "Epoch: [9/10], Step: [101/391], Validation Acc: 64.2\n",
            "Epoch: [9/10], Step: [201/391], Validation Acc: 64.0\n",
            "Epoch: [9/10], Step: [301/391], Validation Acc: 65.4\n",
            "Epoch:9, Training Loss:0.21556690046310426, Training Acc: 94.177\n",
            "Epoch: [10/10], Step: [101/391], Validation Acc: 65.9\n",
            "Epoch: [10/10], Step: [201/391], Validation Acc: 64.5\n",
            "Epoch: [10/10], Step: [301/391], Validation Acc: 65.0\n",
            "Epoch:10, Training Loss:0.18677682888031005, Training Acc: 94.889\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Cx5zl1SJ5LI1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "outputId": "cd922b71-a4ca-49f2-e7b7-855e5101fb45"
      },
      "source": [
        "emb_dim = 100\n",
        "model1 = BagOfWords_Model(len(id2token), emb_dim, nn_ind=True, interaction_type = 'sum').to(device)\n",
        "criterion1 = torch.nn.CrossEntropyLoss(reduction='sum')  \n",
        "\n",
        "learning_rate = 0.01\n",
        "optimizer1 = torch.optim.Adam(model1.parameters(), lr=learning_rate)\n",
        "train_model(train_loader, val_loader, model1, optimizer1, criterion1, num_epochs)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: [1/10], Step: [101/391], Validation Acc: 52.3\n",
            "Epoch: [1/10], Step: [201/391], Validation Acc: 55.6\n",
            "Epoch: [1/10], Step: [301/391], Validation Acc: 55.7\n",
            "Epoch:1, Training Loss:0.9801254168701172, Training Acc: 60.612\n",
            "Epoch: [2/10], Step: [101/391], Validation Acc: 57.3\n",
            "Epoch: [2/10], Step: [201/391], Validation Acc: 56.8\n",
            "Epoch: [2/10], Step: [301/391], Validation Acc: 57.2\n",
            "Epoch:2, Training Loss:0.8742438984680175, Training Acc: 65.488\n",
            "Epoch: [3/10], Step: [101/391], Validation Acc: 57.5\n",
            "Epoch: [3/10], Step: [201/391], Validation Acc: 57.9\n",
            "Epoch: [3/10], Step: [301/391], Validation Acc: 57.1\n",
            "Epoch:3, Training Loss:0.800897191619873, Training Acc: 69.927\n",
            "Epoch: [4/10], Step: [101/391], Validation Acc: 58.5\n",
            "Epoch: [4/10], Step: [201/391], Validation Acc: 58.5\n",
            "Epoch: [4/10], Step: [301/391], Validation Acc: 59.9\n",
            "Epoch:4, Training Loss:0.7313453771972657, Training Acc: 73.542\n",
            "Epoch: [5/10], Step: [101/391], Validation Acc: 58.3\n",
            "Epoch: [5/10], Step: [201/391], Validation Acc: 58.6\n",
            "Epoch: [5/10], Step: [301/391], Validation Acc: 59.5\n",
            "Epoch:5, Training Loss:0.6627115828704834, Training Acc: 76.619\n",
            "Epoch: [6/10], Step: [101/391], Validation Acc: 57.7\n",
            "Epoch: [6/10], Step: [201/391], Validation Acc: 58.1\n",
            "Epoch: [6/10], Step: [301/391], Validation Acc: 56.9\n",
            "Epoch:6, Training Loss:0.598505876083374, Training Acc: 79.335\n",
            "Epoch: [7/10], Step: [101/391], Validation Acc: 56.6\n",
            "Epoch: [7/10], Step: [201/391], Validation Acc: 57.3\n",
            "Epoch: [7/10], Step: [301/391], Validation Acc: 58.8\n",
            "Epoch:7, Training Loss:0.5407173295593262, Training Acc: 82.004\n",
            "Epoch: [8/10], Step: [101/391], Validation Acc: 57.8\n",
            "Epoch: [8/10], Step: [201/391], Validation Acc: 57.6\n",
            "Epoch: [8/10], Step: [301/391], Validation Acc: 58.0\n",
            "Epoch:8, Training Loss:0.484068747177124, Training Acc: 84.122\n",
            "Epoch: [9/10], Step: [101/391], Validation Acc: 58.0\n",
            "Epoch: [9/10], Step: [201/391], Validation Acc: 57.1\n",
            "Epoch: [9/10], Step: [301/391], Validation Acc: 57.3\n",
            "Epoch:9, Training Loss:0.4384103978729248, Training Acc: 85.75\n",
            "Epoch: [10/10], Step: [101/391], Validation Acc: 57.6\n",
            "Epoch: [10/10], Step: [201/391], Validation Acc: 57.4\n",
            "Epoch: [10/10], Step: [301/391], Validation Acc: 56.9\n",
            "Epoch:10, Training Loss:0.3986861543273926, Training Acc: 86.539\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "38c76295-c173-4704-9034-f0f82677efb2",
        "id": "qwLyUORv5LI7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        }
      },
      "source": [
        "emb_dim = 100\n",
        "model1 = BagOfWords_Model(len(id2token), emb_dim, nn_ind=True, interaction_type = 'mult').to(device)\n",
        "criterion1 = torch.nn.CrossEntropyLoss(reduction='sum')  \n",
        "\n",
        "learning_rate = 0.01\n",
        "optimizer1 = torch.optim.Adam(model1.parameters(), lr=learning_rate)\n",
        "train_model(train_loader, val_loader, model1, optimizer1, criterion1, num_epochs)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: [1/10], Step: [101/391], Validation Acc: 50.8\n",
            "Epoch: [1/10], Step: [201/391], Validation Acc: 55.9\n",
            "Epoch: [1/10], Step: [301/391], Validation Acc: 59.5\n",
            "Epoch:1, Training Loss:0.9530219715881347, Training Acc: 67.455\n",
            "Epoch: [2/10], Step: [101/391], Validation Acc: 63.7\n",
            "Epoch: [2/10], Step: [201/391], Validation Acc: 62.6\n",
            "Epoch: [2/10], Step: [301/391], Validation Acc: 63.9\n",
            "Epoch:2, Training Loss:0.7300693878173828, Training Acc: 78.423\n",
            "Epoch: [3/10], Step: [101/391], Validation Acc: 65.0\n",
            "Epoch: [3/10], Step: [201/391], Validation Acc: 65.1\n",
            "Epoch: [3/10], Step: [301/391], Validation Acc: 64.1\n",
            "Epoch:3, Training Loss:0.544019750289917, Training Acc: 85.914\n",
            "Epoch: [4/10], Step: [101/391], Validation Acc: 64.1\n",
            "Epoch: [4/10], Step: [201/391], Validation Acc: 64.0\n",
            "Epoch: [4/10], Step: [301/391], Validation Acc: 65.0\n",
            "Epoch:4, Training Loss:0.38841200874328613, Training Acc: 90.157\n",
            "Epoch: [5/10], Step: [101/391], Validation Acc: 63.0\n",
            "Epoch: [5/10], Step: [201/391], Validation Acc: 63.8\n",
            "Epoch: [5/10], Step: [301/391], Validation Acc: 65.0\n",
            "Epoch:5, Training Loss:0.27807136611938477, Training Acc: 93.503\n",
            "Epoch: [6/10], Step: [101/391], Validation Acc: 62.4\n",
            "Epoch: [6/10], Step: [201/391], Validation Acc: 61.5\n",
            "Epoch: [6/10], Step: [301/391], Validation Acc: 61.5\n",
            "Epoch:6, Training Loss:0.20160266025543214, Training Acc: 95.364\n",
            "Epoch: [7/10], Step: [101/391], Validation Acc: 63.6\n",
            "Epoch: [7/10], Step: [201/391], Validation Acc: 61.9\n",
            "Epoch: [7/10], Step: [301/391], Validation Acc: 62.3\n",
            "Epoch:7, Training Loss:0.15300618082046508, Training Acc: 96.61\n",
            "Epoch: [8/10], Step: [101/391], Validation Acc: 62.6\n",
            "Epoch: [8/10], Step: [201/391], Validation Acc: 63.3\n",
            "Epoch: [8/10], Step: [301/391], Validation Acc: 63.3\n",
            "Epoch:8, Training Loss:0.11754612738609314, Training Acc: 97.296\n",
            "Epoch: [9/10], Step: [101/391], Validation Acc: 62.5\n",
            "Epoch: [9/10], Step: [201/391], Validation Acc: 62.7\n",
            "Epoch: [9/10], Step: [301/391], Validation Acc: 62.3\n",
            "Epoch:9, Training Loss:0.09543609895706177, Training Acc: 97.846\n",
            "Epoch: [10/10], Step: [101/391], Validation Acc: 61.8\n",
            "Epoch: [10/10], Step: [201/391], Validation Acc: 61.6\n",
            "Epoch: [10/10], Step: [301/391], Validation Acc: 62.2\n",
            "Epoch:10, Training Loss:0.079479196600914, Training Acc: 98.274\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9t3kjUzCJBjb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLUI5Kg9ofVh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJWb3tCAs4NT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}